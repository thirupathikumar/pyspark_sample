To compare two PySpark DataFrames and get the columns with mismatched values, you can follow the below steps:

Use the exceptAll() function to get the rows that exist in one DataFrame but not the other. This function returns a new DataFrame that contains only the rows that are in the first DataFrame but not in the second DataFrame.
Use the join() function to join the original DataFrames with the result of the exceptAll() function. This will give you a new DataFrame that contains only the rows with mismatched values.
Use the columns attribute of the resulting DataFrame to get the columns with mismatched values.
Finally, you can use the show() function to display the mismatched values.
Here's an example code snippet:



from pyspark.sql.functions import col

# define two DataFrames to compare
df1 = spark.createDataFrame([(1, "John", "Doe"), (2, "Jane", "Doe")], ["id", "first_name", "last_name"])
df2 = spark.createDataFrame([(1, "John", "Doe"), (2, "Jack", "Smith")], ["id", "first_name", "last_name"])

# get the rows with mismatched values
mismatched_rows = df1.exceptAll(df2)

# join the original DataFrames with the mismatched rows
mismatched_df = df1.join(mismatched_rows, on=[col(x) for x in df1.columns])

# get the columns with mismatched values
mismatched_columns = mismatched_df.columns

# display the mismatched values
mismatched_df.show()



The output

+---+----------+---------+
| id|first_name|last_name|
+---+----------+---------+
|  2|      Jane|      Doe|
|  2|      Jack|    Smith|
+---+----------+---------+



I apologize for the confusion in my previous response. To get only the columns with mismatched values, you can use the except() function instead of exceptAll(). The except() function returns only the distinct rows from the first DataFrame that are not in the second DataFrame. Here's an updated code snippet:


from pyspark.sql.functions import col, array, struct

# define two DataFrames to compare
df1 = spark.createDataFrame([(1, "John", "Doe"), (2, "Jane", "Doe")], ["id", "first_name", "last_name"])
df2 = spark.createDataFrame([(1, "John", "Doe"), (2, "Jack", "Smith")], ["id", "first_name", "last_name"])

# get the rows with mismatched values
mismatched_rows = df1.except(df2)

# get the columns with mismatched values
mismatched_columns = [c for c in df1.columns if c not in ["id"]] # exclude the id column as it is the key column

# create an array of struct for each mismatched column with its value
mismatched_structs = [struct(col(c).alias(c), col(c)).alias(c) for c in mismatched_columns]

# select the key column and the array of struct for each mismatched column
mismatched_df = mismatched_rows.select("id", array(*mismatched_structs).alias("mismatched_columns"))

# display the mismatched columns and their values
mismatched_df.show(truncate=False)



The output 

+---+-----------------------------------------+
|id |mismatched_columns                       |
+---+-----------------------------------------+
|2  |[{first_name, Jane}, {last_name, Doe}]   |
|2  |[{first_name, Jack}, {last_name, Smith}]|
+---+-----------------------------------------+



In this example, the mismatched_columns column is an array of struct that contains the name of each mismatched column and its corresponding value for each row with mismatched values.





Recently I needed to check for equality between Pyspark dataframes as part of a test suite. To my surprise I discovered that there is no built in function to test for dataframe equality.

It turns out that checking dataframe equality in PySpark  is not a trivial issue.  Looking at the problem the except command which subtracts one dataframe from another looks like a promising approach since it will deal with structured data columns. Naively you night think you could simply write a function to subtract one dataframe from the other and check the result is empty:

def are_dataframes_equal(df_actual, df_expected): 
  return df_actual.subtract(df_expected).rdd.isEmpty() 
However this will fail if df_actual contains more rows than df_expected. We can avoid that pitfall by checking both ways round

def are_dataframes_equal(df_actual, df_expected): 
  if df_actual.subtract(df_expected).rdd.isEmpty():
    return df_expected.subtract(df_actual).rdd.isEmpty()
  return False
Even this solution can still run into problems due to duplicate rows. A good way to avoid this is to do a group by and count on all columns of each dataframe. The resulting count column will differ if the two dataframes do not have the same row duplication. This gives us a function like:

def are_dataframes_equal(df_actual, df_expected): 
  # sorts are needed in case if disordered columns
  a_cols = sorted(df_actual.columns)
  e_cols = sorted(df_expected.columns)
  # we don't know the column names so count on the first column we find
  df_a = df_actual.groupby(a_cols).agg(fn.count(a_cols[1]))
  df_e = df_expected.groupby(e_cols).agg(fn.count(e_cols[1]))
  # then perform our equality checks on the dataframes with the row counts
  if df_a.subtract(df_e).rdd.isEmpty():
    return df_e.subtract(df_a).rdd.isEmpty()
  return False
Finally we need to be able to handle schema mismatches. Fortunately for us it turns out that the above code smoothly handles both differing column names and differing column types correctly. This solution also handles struct and array columns nicely. However caution should still be observed around map columns. These may run into issues if the elements of the map are disordered. Further work may be required to sort the map columns prior to comparison.

This function can be slow to run on large dataframes, but is very useful for testing purposes. You can find more on PySpark testing  here. Some further notes including another approach to dataframe equality can be found here

A workbook with the equality checking functions discussed above, and some example pass and failure cases, can be found on GitHub

UPDATE: If you are looking for a good library for checking PySpark dataframe equality you could try Chispa by MrPowers which is available on Github  https://github.com/MrPowers/chispa


https://github.com/JustinMatters/pyspark-example-workbooks








import pyspark.sql.functions as fn
#from pyspark.sql.types import StringType, IntegerType, DecimalType, FloatType, LongType, DoubleType, StructType, StructField
import pandas as pd
     
Lets start with a naive dataframe comparison and work up
In [3]:

values_1 = {
  "String": ["one", "two", "three",],
  "Value": [1, 2, 3,],
}

values_2 = {
  "String": ["one", "two",],
  "Value": [1, 2, ],
}

df_1 = spark.createDataFrame(
  pd.DataFrame.from_dict(values_1)
)

df_2 = spark.createDataFrame(
  pd.DataFrame.from_dict(values_2)
)
     
In [4]:

# naive function
def are_dataframes_equal(df_actual, df_expected): 
  return df_actual.subtract(df_expected).rdd.isEmpty()

print(are_dataframes_equal(df_1, df_2)) # returns False correctly
print(are_dataframes_equal(df_2, df_1)) # returns True INCORRECTLY
     
False True
In [5]:

# improved function
def are_dataframes_equal(df_actual, df_expected): 
  if df_actual.subtract(df_expected).rdd.isEmpty():
    return df_expected.subtract(df_actual).rdd.isEmpty()
  return False

print(are_dataframes_equal(df_1, df_2)) # returns False correctly
print(are_dataframes_equal(df_2, df_1)) # returns False correctly
print(are_dataframes_equal(df_2, df_2)) # returns True correctly
     
False False True
What if we have duplicate rows?
In [7]:

values_3 = {
  "String": ["one", "two", "two",],
  "Value": [1, 2, 2,],
}

values_4 = {
  "String": ["one", "one", "two",],
  "Value": [1, 1, 2, ],
}

df_3 = spark.createDataFrame(
  pd.DataFrame.from_dict(values_3)
)

df_4 = spark.createDataFrame(
  pd.DataFrame.from_dict(values_4)
)
     
In [8]:

# this still fools our improved equality check
print(are_dataframes_equal(df_1, df_3)) # returns False correctly
print(are_dataframes_equal(df_2, df_3)) # returns True INCORRECTLY
print(are_dataframes_equal(df_4, df_3)) # returns True INCORRECTLY
     
False True True
In [9]:

# lets use a groupby to improve on this further

def are_dataframes_equal(df_actual, df_expected): 
  # sorts are needed in case if disordered columns
  a_cols = sorted(df_actual.columns)
  e_cols = sorted(df_expected.columns)
  # we don't know the column names so just count on the first column we find
  df_a = df_actual.groupby(a_cols).agg(fn.count(a_cols[1]))
  df_e = df_expected.groupby(e_cols).agg(fn.count(e_cols[1]))
  # then perform our equality checks on the dataframes with the row counts
  if df_a.subtract(df_e).rdd.isEmpty():
    return df_e.subtract(df_a).rdd.isEmpty()
  return False
     
In [10]:

# this still fools our improved equality check
print(are_dataframes_equal(df_1, df_3)) # returns False correctly
print(are_dataframes_equal(df_2, df_3)) # returns False correctly
print(are_dataframes_equal(df_4, df_3)) # returns False correctly
print(are_dataframes_equal(df_1, df_1)) # returns True correctly
print(are_dataframes_equal(df_4, df_4)) # returns True correctly
     
False False False True True
Finally we need to check we can handle schema mismatches without crashing
In [12]:

# check differing column names works
values_5= {
  "String": ["one", "two", "three",],
  "Property": [1, 2, 3,],
}

df_5 = spark.createDataFrame(
  pd.DataFrame.from_dict(values_5)
)

# check differing column types work
values_6= {
  "String": [1,2,3,],
  "Value": [1, 2, 3,],
}

df_6 = spark.createDataFrame(
  pd.DataFrame.from_dict(values_6)
)

     
In [13]:

print(are_dataframes_equal(df_1, df_5)) # returns False correctly
print(are_dataframes_equal(df_1, df_6)) # returns False correctly
     
False False
Be aware that unsorted map columns may incorrectly return false when compared





https://github.com/JustinMatters/pyspark-example-workbooks/blob/master/Equality%20Checks%20in%20Pyspark.ipynb



Regards, 
Thiruppathi