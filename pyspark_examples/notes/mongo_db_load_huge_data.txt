Mongo DB Error and Solution:
-----------------------------

https://stackoverflow.com/questions/38096502/stack-overflow-error-when-loading-a-large-table-from-mongodb-to-spark

I add another java option "-Xss32m" to spark driver to raise the memory of stack for every thread , and this exception is not throwing any more. How stupid was I , I should have tried it earlier. But another problem is shown, I will have to check more. still great thanks for your help.



mongo db write config - writeConcern.w "majority".

“majority” means Write operation returns acknowledgement after propagating to M-number of data-bearing voting members (primary and secondaries)

Possible writeConcern.w option:
----------------------------------
"majority" - Requests acknowledgment that write operations have propagated to the calculated majority of the data-bearing voting members (i.e. primary and secondaries with members[n].votes greater than 0). { w: "majority" } is the default write concern for most MongoDB deployments. For example, consider a replica set with 3 voting members, Primary-Secondary-Secondary (P-S-S). For this replica set, 
calculated majority is two, and the write must propagate to the primary and one secondary to acknowledge the write concern to the client.

After the write operation returns with a w: "majority" acknowledgment to the client, the client can read the result of that write with a "majority" readConcern. If you specify a "majority" write concern for a multi-document transaction and the transaction fails to replicate to the calculated majority of replica set members, then the transaction may not immediately roll back on replica set members. The replica set will be eventually consistent. A transaction is always applied or rolled back on all replica set members.

<number>: Requests acknowledgment that the write operation has propagated to the specified number of mongod instances. For example:
w: 1 - Requests acknowledgment that the write operation has propagated to the standalone mongod or the primary in a replica set. Data can be rolled back if the primary steps down before the write operations have replicated to any of the secondaries.
w: 0 - Requests no acknowledgment of the write operation. However, w: 0 may return information about socket exceptions and networking errors to the application. Data can be rolled back if the primary steps down before the write operations have replicated to any of the secondaries.

If you specify w: 0 but include j: true, the j: true prevails to request acknowledgment from the standalone mongod or the primary of a replica set.w greater than 1 requires acknowledgment from the primary and as many data-bearing secondaries as needed to meet the specified write concern. For example, consider a 3-member replica set with a primary and 2 secondaries. Specifying w: 2 would require acknowledgment from the primary and one of the secondaries. Specifying w: 3 would require acknowledgment from the primary and both secondaries.

custom write concern name - Requests acknowledgment that the write operations have propagated to tagged members that satisfy the custom write concern defined in settings.getLastErrorModes. For an example, see Custom Multi-Datacenter Write Concerns.Data can be rolled back if the custom write concern only requires acknowledgment from the primary and the primary steps down before the write operations have replicated to any of the secondaries.

******************************************************************************************************************************************

https://medium.com/@thomaspt748/how-to-load-millions-of-data-into-mongo-db-using-apache-spark-3-0-8bcf089bd6ed
Load millions of data into MongoDB using apache spark 3.0 


Ref - https://www.mongodb.com/docs/manual/reference/write-concern/#std-label-wc-w

"","CASE_STATUS","EMPLOYER_NAME","SOC_NAME","JOB_TITLE","FULL_TIME_POSITION","PREVAILING_WAGE","YEAR","WORKSITE","lon","lat"
"1","CERTIFIED-WITHDRAWN","UNIVERSITY OF MICHIGAN","BIOCHEMISTS AND BIOPHYSICISTS","POSTDOCTORAL RESEARCH FELLOW","N",36067,2016,"ANN ARBOR, MICHIGAN",-83.7430378,42.2808256
"2","CERTIFIED-WITHDRAWN","GOODMAN NETWORKS, INC.","CHIEF EXECUTIVES","CHIEF OPERATING OFFICER","Y",242674,2016,"PLANO, TEXAS",-96.6988856,33.0198431
"3","CERTIFIED-WITHDRAWN","PORTS AMERICA GROUP, INC.","CHIEF EXECUTIVES","CHIEF PROCESS OFFICER","Y",193066,2016,"JERSEY CITY, NEW JERSEY",-74.0776417,40.7281575
"4","CERTIFIED-WITHDRAWN","GATES CORPORATION, A WHOLLY-OWNED SUBSIDIARY OF TOMKINS PLC","CHIEF EXECUTIVES","REGIONAL PRESIDEN, AMERICAS","Y",220314,2016,"DENVER, COLORADO",-104.990251,39.7392358
"5","WITHDRAWN","PEABODY INVESTMENTS CORP.","CHIEF EXECUTIVES","PRESIDENT MONGOLIA AND INDIA","Y",157518.4,2016,"ST. LOUIS, MISSOURI",-90.1994042,38.6270025
"6","CERTIFIED-WITHDRAWN","BURGER KING CORPORATION","CHIEF EXECUTIVES","EXECUTIVE V P, GLOBAL DEVELOPMENT AND PRESIDENT, LATIN AMERI","Y",225000,2016,"MIAMI, FLORIDA",-80.1917902,25.7616798
"7","CERTIFIED-WITHDRAWN","BT AND MK ENERGY AND COMMODITIES","CHIEF EXECUTIVES","CHIEF OPERATING OFFICER","Y",91021,2016,"HOUSTON, TEXAS",-95.3698028,29.7604267
"8","CERTIFIED-WITHDRAWN","GLOBO MOBILE TECHNOLOGIES, INC.","CHIEF EXECUTIVES","CHIEF OPERATIONS OFFICER","Y",150000,2016,"SAN JOSE, CALIFORNIA",-121.8863286,37.3382082
"9","CERTIFIED-WITHDRAWN","ESI COMPANIES INC.","CHIEF EXECUTIVES","PRESIDENT","Y",127546,2016,"MEMPHIS, TEXAS",NA,NA
"10","WITHDRAWN","LESSARD INTERNATIONAL LLC","CHIEF EXECUTIVES","PRESIDENT","Y",154648,2016,"VIENNA, VIRGINIA",-77.2652604,38.9012225

dataDf.rdd.mapPartitions( partition => {
	val docs = partition.map(row => {
        
        var colMap: Map[String, String] = Map()
        
        row.schema.map(_.name).map(col => {
            val colName = col.trim
            val colIndex = row.fieldIndex(colName)
            val colAnyVal = row.getAs[AnyVal](colIndex)
            var colVal = if (colAnyVal == null) "NULL" else colAnyVal.toString()
            if (colVal == null){
                val nullCol = "NULL"
                colVal = nullCol.asInstanceOf[String]
            }
            
            if (colVal.toString() == "NULL") {
                //Do nothings
            }
            else {
                colMap = colMap ++ Map(colName -> colVal)
            }
        
        }
         val json = JSONObject(colMap).toString()
         val parsedJSON = Document.parse(json)
         parsedJson    
	}
    ).toIterator
    docs
}
)

Note: If all the column has the proper value then we can store the data into the using spark dataframe. But assume if any of the column has the the NULL value or does not has value then we need to ignore that is being stored in the mongo db, to use that we have to go with above defined approach (my guess) 


Sparse Index and Null values in MongoDB:
-------------------------------------------
Sparse indexes do not contain documents that miss indexed field. However, if field exists and has value of null, it will still be indexed. So, if absense of the field and its equality to null look the same for your application and you want to maintain uniqueness of fbId, just don't insert it until you have a value for it.

You need sparse indexes when you have a large number of documents, but only a small portion of them contains some field, and you want to be able to quickly find documents by that field. Creating a normal index would be too expensive, you would just waste precious RAM on indexing documents you're not interested in.

Ref - https://stackoverflow.com/questions/8608567/sparse-indexes-and-null-values-in-mongo
https://www.percona.com/blog/using-partial-and-sparse-indexes-in-mongodb/


Ref- https://stackoverflow.com/questions/59486174/what-is-sparse-what-is-the-purpose-of-sparse-in-mongodb
Sparse indexes are like regular indexes in mongodb. The differece is that sparse indexes only include documents that have the indexed field, whereas regular indexdes include all documents no matter if the indexded field exist or not.

For example, when you create an index on a age field of customers collection that not every customer has a age info on file, sparse index will exclude customers don’t have a age field whereas the regular index will include all customers even if the customer don’t have a value for age. Imagine there is a billion of documents in the customer collection and half of them don’t have age on file, a sparse index on the age will save a lot of memory spaces.

Create a sparse index on the age of the customers collection by adding {sparse: true}

db.customers.createIndex( { age: 1 }, { sparse: true } );

This query will use the sparse index because the criteria in the query goes well with the sparse index. We only want to return customers whose age is greater than 21, if the customer doens’t have age on file, we don’t include them.

db.customers.find({age: {$gt:21}});

This query will not use the sparse index, it will return all documents including those don’t have age field.
db.customers.sort({age:1});
This query will use the sparse index, it will only return documents that have age field.

db.customers.sort({age:1}).hint({age:1});

https://kumardeep245.medium.com/partial-indexing-vs-sparse-indexing-in-mongo-database-c7e213b6cfde










****************************************************************************************************************************************
Get schema of mongodb collection:
https://medium.com/@ahsan.ayaz/how-to-find-schema-of-a-collection-in-mongodb-d9a91839d992

MongoDB shell version: 2.4.5
> show dbs
local 0.078125GB
todo 0.453125GB
> use todo
switched to db todo
> show collections
system.indexes
tasks
> var schematodo = db.tasks.findOne();
> for (var key in schematodo) { print (key) ; }
_id
label
content

Spark mongo db Structured Streaming - https://www.mongodb.com/docs/spark-connector/current/structured-streaming/

****************************************************************************************************************************************

Mongo Dynamic Schema Mapping:
------------------------------
from pyspark import SparkContext, SparkConf
from pyspark.sql import SQLContext
from pyspark.sql.types import StructType, StructField, StringType
import pymongo_spark 
pymongo_spark.activate()
data_rdd = sc.mongoRDD("mongodb://localhost:27017/database.collection")
sqlcontext = SQLContext(sc)

# Define your schema explicitly
schema = StructType([StructField("firstname", StringType()),
                     StructField("lastname", StringType()),
                     StructField("description", StringType())])

# Create a mapper function to return only the fields wanted, or to convert. 
def project(doc):
    return {"firstname": str(doc["firstname"]), 
            "lastname": str(doc["lastname"]), 
            "description": str(doc["description"])}

projected_rdd = data_rdd.map(project)
train_df = sqlcontext.jsonRDD(projected_rdd, schema)
train_df.first()



****************************************************************************************************************************************

https://lightrun.com/answers/trinodb-trino-mongodb-implicit-schema-inferred-from-data-is-not-refreshed-after-data-changes


Get the complete schema of the mongo db collection using python
In this example, we connect to MongoDB using the pymongo.MongoClient class and specify the MongoDB URI. We then get a reference to the "test" database and the "people" collection.

We then define a function called flatten_dict that takes a nested dictionary as input and returns a flattened version of the dictionary. This function is used to flatten the nested documents in the collection.

Next, we define a function called get_schema that takes a collection as input and returns the schema of the collection. This function aggregates the fields of all documents in the collection, and keeps track of the types of values in each field.

Finally, we call the get_schema function on the "people" collection to get the schema of the collection, and print the schema.

Note that this approach is just a general representation of the schema of a MongoDB collection, and may not accurately reflect the complete schema of the collection in all cases.

import pymongo

# Connect to MongoDB
client = pymongo.MongoClient("mongodb://localhost:27017/")

# Get a reference to the database
db = client["test"]

# Get a reference to the collection
collection = db["people"]

# Define a function to flatten a nested dictionary
def flatten_dict(d, parent_key="", sep="_"):
    items = []
    for k, v in d.items():
        new_key = parent_key + sep + k if parent_key else k
        if isinstance(v, dict):
            items.extend(flatten_dict(v, new_key, sep=sep).items())
        else:
            items.append((new_key, v))
    return dict(items)

# Define a function to get the schema of a collection
def get_schema(collection):
    schema = {}
    for document in collection.find():
        flat_doc = flatten_dict(document)
        for field, value in flat_doc.items():
            if field not in schema:
                schema[field] = type(value)
    return schema

# Get the schema of the collection
schema = get_schema(collection)

# Print the schema
print(schema)


How to find the schema of a mongo db collection - 
https://medium.com/@ahsan.ayaz/how-to-find-schema-of-a-collection-in-mongodb-d9a91839d992


Pyspark complex JSON handling:
https://www.youtube.com/watch?v=1dEflnG0iho

json explode function 


****************************************************************************************************************************************