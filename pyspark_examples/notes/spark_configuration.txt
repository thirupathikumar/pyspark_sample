#Reference - https://spark.apache.org/docs/latest/configuration.html#runtime-environment
Spark provides three locations to configure the system:
1.Spark properties control most application parameters and can be set by using a SparkConf object, or through Java system properties.
2.Environment variables can be used to set per-machine settings, such as the IP address, through the conf/spark-env.sh script on each node.
3.Logging can be configured through log4j2.properties.

Dynamically Loading Spark Properties:
In some cases, you may want to avoid hard-coding certain configurations in a SparkConf. For instance, if you’d like to run the same application with different masters or different amounts of memory. Spark allows you to simply create an empty conf:

val sc = new SparkContext(new SparkConf())

Then, you can supply configuration values at runtime:

./bin/spark-submit --name "My app" --master local[4] --conf spark.eventLog.enabled=false
  --conf "spark.executor.extraJavaOptions=-XX:+PrintGCDetails -XX:+PrintGCTimeStamps" myApp.jar

The name and the master is the command line option that we can pass shown above. 
We can pass the spark properties using --conf or -c flag. 
But we have to the special flag for properties that play a part in launching the spark application.Running ./bin/spark-submit --help will show the entire list of these options.

The bin/spark-submit will also read the configuration option from conf/spark-defaults.conf, in which each line consists of a key and a value separated by whitespace for example.
spark.master            spark://5.6.7.8:7077
spark.executor.memory   4g
spark.eventLog.enabled  true
spark.serializer        org.apache.spark.serializer.KryoSerializer
 
Precedence:
The below precedence will be applied 
The values specified as flag (using -conf or -c) and the values in the spark-defaults.conf will be passed on to the application through spark conf
1.Properties set directly on the SparkConf (The values set in the code using sparkconf) get the highest precedence
2.The second prcedence is for then flags passed to spark-submit or spark-shell using --conf or -c
3.The last precednce is for the spark-defaults.conf file

The best practices would be pass the properties value using --conf option in the spark-submit. Otherwise have the configuration file and read that from the spark application and using that values with spark conf and in case any changes we have to change the value in the configuration file and this configuration file can be used by the multiple applications. 

Viewing Spark Properties:
The application web UI at http://<driver>:4040 list spark properties in the Environment tab.Note that only values explicitly specified through spark-defaults.conf, SparkConf, or the command line will appear. For all other configuration properties, you can assume the default value is used.

Important Properties:
Property Name  Default and Meaning 
spark.app.name	(none)	The name of your application. This will appear in the UI and in log data.

spark.driver.cores	1	Number of cores to use for the driver process, only in cluster mode.

spark.driver.memory	1g	Amount of memory to use for the driver process, i.e. where SparkContext is initialized, in the same format as JVM memory strings with a size unit suffix ("k", "m", "g" or "t") (e.g. 512m, 2g).
Note: In client mode, this config must not be set through the SparkConf directly in your application, because the driver JVM has already started at that point. Instead, please set this through the --driver-memory command line option or in your default properties file.

spark.driver.memoryOverhead
driverMemory * spark.driver.memoryOverheadFactor, with minimum of 384
Amount of non-heap memory to be allocated per driver process in cluster mode, in MiB unless otherwise specified. This is memory that accounts for things like VM overheads, interned strings, other native overheads, etc. This tends to grow with the container size (typically 6-10%). This option is currently supported on YARN, Mesos and Kubernetes. Note: Non-heap memory includes off-heap memory (when spark.memory.offHeap.enabled=true) and memory used by other driver processes (e.g. python process that goes with a PySpark driver) and memory used by other non-driver processes running in the same container. The maximum memory size of container to running driver is determined by the sum of spark.driver.memoryOverhead and spark.driver.memory.

Runtime Environment Properties:
spark.driver.extraClassPath
(none)
Extra classpath entries to prepend to the classpath of the driver.
Note: In client mode, this config must not be set through the SparkConf directly in your application, because the driver JVM has already started at that point. Instead, please set this through the --driver-class-path command line option or in your default properties file.

spark.driver.extraLibraryPath
(none)
Set a special library path to use when launching the driver JVM.
Note: In client mode, this config must not be set through the SparkConf directly in your application, because the driver JVM has already started at that point. Instead, please set this through the --driver-library-path command line option or in your default properties file.


spark.executor.extraClassPath
(none)
Extra classpath entries to prepend to the classpath of executors. This exists primarily for backwards-compatibility with older versions of Spark. Users typically should not need to set this option.

spark.executor.extraLibraryPath
(none)
Set a special library path to use when launching executor JVM's.

spark.files
(none)
Comma-separated list of files to be placed in the working directory of each executor. Globs are allowed.

spark.submit.pyFiles
(none)
Comma-separated list of .zip, .egg, or .py files to place on the PYTHONPATH for Python apps. Globs are allowed.

spark.jars
(none)
Comma-separated list of jars to include on the driver and executor classpaths. Globs are allowed.


spark.jars.packages
(none)
Comma-separated list of Maven coordinates of jars to include on the driver and executor classpaths. The coordinates should be groupId:artifactId:version. If spark.jars.ivySettings is given artifacts will be resolved according to the configuration in the file, otherwise artifacts will be searched for in the local maven repo, then maven central and finally any additional remote repositories given by the command-line option --repositories. For more details, see Advanced Dependency Management.

spark.jars.excludes
Comma-separated list of groupId:artifactId, to exclude while resolving the dependencies provided in spark.jars.packages to avoid dependency conflicts.

spark.pyspark.driver.python
Python binary executable to use for PySpark in driver. (default is spark.pyspark.python)

spark.pyspark.python
Python binary executable to use for PySpark in both driver and executors.

spark.driver.defaultJavaOptions
(none)
A string of default JVM options to prepend to spark.driver.extraJavaOptions. This is intended to be set by administrators. For instance, GC settings or other logging. Note that it is illegal to set maximum heap size (-Xmx) settings with this option. Maximum heap size settings can be set with spark.driver.memory in the cluster mode and through the --driver-memory command line option in the client mode.
Note: In client mode, this config must not be set through the SparkConf directly in your application, because the driver JVM has already started at that point. Instead, please set this through the --driver-java-options command line option or in your default properties file.


spark.driver.extraJavaOptions
(none)
A string of extra JVM options to pass to the driver. This is intended to be set by users. For instance, GC settings or other logging. Note that it is illegal to set maximum heap size (-Xmx) settings with this option. Maximum heap size settings can be set with spark.driver.memory in the cluster mode and through the --driver-memory command line option in the client mode.
Note: In client mode, this config must not be set through the SparkConf directly in your application, because the driver JVM has already started at that point. Instead, please set this through the --driver-java-options command line option or in your default properties file. spark.driver.defaultJavaOptions will be prepended to this configuration.


Adding Jar to Spark Submit Classpath:
When submitting Spark or PySpark application using spark-submit, we often need to include multiple third-party jars in classpath, Spark supports multiple ways to add dependency jars to the classpath.

1.Creating uber or assembly jar - Create an assembly or uber jar by including your application classes and all third-party dependencies. You can do this either using the Maven shade plugin or equivalent SBT assembly, for PySpark create a zip file or egg file.By doing this, you don’t have to worry about adding jars to the classpath as all dependencies are already part of your uber jar.


2.Adding Individual Jar to the classpath - Adding multiple third-party jars to classpath can be done using spark-submit, spark-defaults.conf, and SparkConf properties, before using these options you need to understand the priority of how these apply. Below is the precedence of how they apply in order.
1.Properties set directly on the SparkConf take the highest precedence.
2.The second precedence goes to spark-submit options.
3.Finally, properties specified in spark-defaults.conf file.
When you are setting jars in different places, remember the precedence it takes. Use spark-submit with --verbose option to get more details about what jars spark has used.

3.Adding jar to classpath - You can also add jars using Spark submit option--jar, using this option you can add a single jar or multiple jars by comma-separated.
spark-submit --master yarn
             --class com.sparkbyexamples.WordCountExample
             --jars /path/first.jar,/path/second.jar,/path/third.jar
             your-application.jar 
Alternatively, you can also use SparkContext.addJar()

4.Adding all jars from a folder to classpath
If you have many jars, imagine using all these jars in a comma-separated and when you have to update the version of the jars, it’s going to be a nightmare to maintain this.			
You can use the below snippet to add all jars from a folder automatically, $(echo /path/*.jar | tr ' ' ',') statement creates a comma-separated string by appending all jar names in a folder.
spark-submit -- class com.sparkbyexamples.WordCountExample \ 
             --jars $(echo /path/*.jar | tr ' ' ',') \ 
             your-application.jar

5.Adding jar with spark-defaults.conf
You can also specify jars on $SPARK_HOME/conf/spark-defaults.conf, but this is not a preferable option and any libraries you specify here take low precedence.
#Add jars to driver classpath
spark.driver.extraClassPath /path/first.jar:/path/second.jar
#Add jars to executor classpath
spark.executor.extraClassPath /path/first.jar:/path/second.jar

6.Using SparkConf properties - This takes the higher priority among other configs
spark = SparkSession \
        .builder \
        .appName("SparkByExamples.com") \
        .config("spark.yarn.dist.jars", "/path/first.jar,/path/second.jar") \
        .getOrCreate()
		
7.Adding jars to spark driver - Sometimes you may need to add a jar to only Spark driver, you can do this by using --driver-class-path or --conf spark.driver.extraClassPath
spark-submit -- class com.sparkbyexamples.WordCountExample \ 
             --jars $(echo /path/jars/*.jar | tr ' ' ',') \ 
             --driver-class-path jar-driver.jar
             your-application.jar
			 

8.Adding jar to spark-shell - Options on spark-shell are similar to spark-submit hence you can use the options specified above to add one or multiple jars to spark-shell classpath.
spark-shell --driver-class-path /path/to/example.jar:/path/to/another.jar

9.Other option - 
--conf spark.driver.extraLibraryPath=/path/ 
# or use below, both do the same
--driver-library-path /path/

Questions:
What is the use of --driver-class-path in the spark command?
--driver-class-path or spark.driver.extraClassPath can be used for to modify class path only for the Spark driver. This is useful for libraries which are not required by the executors (for example any code that is used only locally).

Compared to that, --jars or spark.jars will not only add jars to both driver and executor classpath, but also distribute archives over the cluster. If particular jar is used only by the driver this is unnecessary overhead.

Note: if I understand well --jars is a superset of --driver-class-path, but in my case on spark 2.4.3 if I only use --jars I get a No suitable driver exception. If I use both --jars and --driver-class-path all runs fine.

Pyspark Add External Jar:
./bin/spark-submit --jars xxx.jar your_spark_script.py

If you have any python dependencies then we have to pass that along with --py-files
pyspark --py-files /path/to/jar/xxxx.jar

From Jypter Notebook 
spark = (SparkSession
    .builder
    .appName("Spark_Test")
    .master('yarn-client')
    .config("spark.sql.warehouse.dir", "/user/hive/warehouse")
    .config("spark.executor.cores", "4")
    .config("spark.executor.instances", "2")
    .config("spark.sql.shuffle.partitions","8")
    .enableHiveSupport()
    .getOrCreate())

# Do this 

spark.sparkContext.addPyFile("/path/to/jar/xxxx.jar")



spark.driver.userClassPathFirst:
Another approach in Apache Spark 2.1.0 is to use --conf spark.driver.userClassPathFirst=true during spark-submit which changes the priority of the dependency load, and thus the behavior of the spark-job, by giving priority to the JAR files the user is adding to the class-path with the --jars option.

*******************************************************************************************************************************************
Ref - https://stackoverflow.com/questions/37132559/add-jar-files-to-a-spark-job-spark-submit
There is a restriction on using --jars: if you want to specify a directory for the location of jar/xml files, it doesn't allow directory expansions. This means if you need to specify an absolute path for each JAR file.

If you specify --driver-class-path and you are executing in yarn cluster mode, then the driver class doesn't get updated. We can verify if the class path is updated or not under the Spark UI or Spark history server under the tab environment.

The option which worked for me to pass JAR files which contains directory expansions and which worked in yarn cluster mode was the --conf option. It's better to pass the driver and executor class paths as --conf, which adds them to the Spark session object itself and those paths are reflected in the Spark configuration. But please make sure to put JAR files on the same path across the cluster.

Summary: Always use --conf spark.driver.extraClassPath and --conf spark.executor.extraClassPath to add the jar file and any other files (The reason is If you specify --driver-class-path and you are executing in yarn cluster mode, then the driver class doesn't get updated. ). When we use --jars if you want to specify a directory for the location of jar/xml files, it doesn't allow directory expansions.
  
Example:
spark-submit \
  --master yarn \
  --queue spark_queue \
  --deploy-mode cluster    \
  --num-executors 12 \
  --executor-memory 4g \
  --driver-memory 8g \
  --executor-cores 4 \
  --conf spark.ui.enabled=False \
  --conf spark.driver.extraClassPath=/usr/hdp/current/hbase-master/lib/hbase-server.jar:/usr/hdp/current/hbase-master/lib/hbase-common.jar:/usr/hdp/current/hbase-master/lib/hbase-client.jar:/usr/hdp/current/hbase-master/lib/zookeeper.jar:/usr/hdp/current/hbase-master/lib/hbase-protocol.jar:/usr/hdp/current/spark2-thriftserver/examples/jars/scopt_2.11-3.3.0.jar:/usr/hdp/current/spark2-thriftserver/examples/jars/spark-examples_2.10-1.1.0.jar:/etc/hbase/conf \
  --conf spark.hadoop.mapred.output.dir=/tmp \
  --conf spark.executor.extraClassPath=/usr/hdp/current/hbase-master/lib/hbase-server.jar:/usr/hdp/current/hbase-master/lib/hbase-common.jar:/usr/hdp/current/hbase-master/lib/hbase-client.jar:/usr/hdp/current/hbase-master/lib/zookeeper.jar:/usr/hdp/current/hbase-master/lib/hbase-protocol.jar:/usr/hdp/current/spark2-thriftserver/examples/jars/scopt_2.11-3.3.0.jar:/usr/hdp/current/spark2-thriftserver/examples/jars/spark-examples_2.10-1.1.0.jar:/etc/hbase/conf \
  --conf spark.hadoop.mapreduce.output.fileoutputformat.outputdir=/tmp
  
*******************************************************************************************************************************************
The below is applicable only when the deploy mode is yarn
spark.yarn.jars - List of libraries containing Spark code to distribute to YARN containers. By default, Spark on YARN will use Spark jars installed locally, but the Spark jars can also be in a world-readable location on HDFS. This allows YARN to cache it on nodes so that it doesn't need to be distributed each time an application runs. To point to jars on HDFS, for example, set this configuration to hdfs:///some/path. Globs are allowed.

spark.yarn.archive - An archive containing needed Spark jars for distribution to the YARN cache. If set, this configuration replaces spark.yarn.jars and the archive is used in all the application's containers. The archive should contain jar files in its root directory. Like with the previous option, the archive can also be hosted on HDFS to speed up file distribution.

*******************************************************************************************************************************************
Accepted URI for files:
---------------------------
When using spark-submit, the application jar along with any jars included with the --jars option will be automatically transferred to the cluster. Spark uses the following URL scheme to allow different strategies for disseminating jars:
file: - Absolute paths and file:/ URIs are served by the driver’s HTTP file server, and every executor pulls the file from the driver HTTP server.
hdfs:, http:, https:, ftp: - these pull down files and JARs from the URI as expected
local: - a URI starting with local:/ is expected to exist as a local file on each worker node. This means that no network IO will be incurred, and works well for large files/JARs that are pushed to each worker, or shared via NFS, GlusterFS, etc.

JAR files are copied to the working directory for each Worker node. Where exactly is that? It is usually under /var/run/spark/work, you'll see them like this:
drwxr-xr-x    3 spark spark   4096 May 15 06:16 app-20160515061614-0027
drwxr-xr-x    3 spark spark   4096 May 15 07:04 app-20160515070442-0028
drwxr-xr-x    3 spark spark   4096 May 15 07:18 app-20160515071819-0029
drwxr-xr-x    3 spark spark   4096 May 15 07:38 app-20160515073852-0030
drwxr-xr-x    3 spark spark   4096 May 15 08:13 app-20160515081350-0031
drwxr-xr-x    3 spark spark   4096 May 18 17:20 app-20160518172020-0032
drwxr-xr-x    3 spark spark   4096 May 18 17:20 app-20160518172045-0033

And when you look inside, you'll see all the JAR files you deployed along:

[*@*]$ cd /var/run/spark/work/app-20160508173423-0014/1/
[*@*]$ ll
total 89988
-rwxr-xr-x 1 spark spark   801117 May  8 17:34 awscala_2.10-0.5.5.jar
-rwxr-xr-x 1 spark spark 29558264 May  8 17:34 aws-java-sdk-1.10.50.jar
-rwxr-xr-x 1 spark spark 59466931 May  8 17:34 com.mycode.code.jar
-rwxr-xr-x 1 spark spark  2308517 May  8 17:34 guava-19.0.jar
-rw-r--r-- 1 spark spark      457 May  8 17:34 stderr
-rw-r--r-- 1 spark spark        0 May  8 17:34 stdout

Different Options:
--jars vs SparkContext.addJar: These are identical. Only one is set through Spark submit and one via code. Choose the one which suits you better. One important thing to note is that using either of these options does not add the JAR file to your driver/executor classpath. You'll need to explicitly add them using the extraClassPath configuration on both.

SparkContext.addJar vs SparkContext.addFile: Use the former when you have a dependency that needs to be used with your code. Use the latter when you simply want to pass an arbitrary file around to your worker nodes, which isn't a run-time dependency in your code.

--conf spark.driver.extraClassPath=... or --driver-class-path: These are aliases, and it doesn't matter which one you choose
--conf spark.driver.extraLibraryPath=..., or --driver-library-path ... Same as above, aliases.

--conf spark.executor.extraClassPath=...: Use this when you have a dependency which can't be included in an über JAR (for example, because there are compile time conflicts between library versions) and which you need to load at runtime.
--conf spark.executor.extraLibraryPath=... This is passed as the java.library.path option for the JVM. Use this when you need a library path visible to the JVM.

Note: Would it be safe to assume that for simplicity, I can add additional application jar files using the 3 main options at the same time:

Summary: The recommendation is add the files in all three options --jars --conf spark.driver.extraClassPath --conf spark.executor.extraClassPath


Example: (Note in the example we have used --driver-class-path instead of --conf spark.driver.extraClassPath and these two are identical as explained above)
spark-submit --jars additional1.jar,additional2.jar \
  --driver-class-path additional1.jar:additional2.jar \
  --conf spark.executor.extraClassPath=additional1.jar:additional2.jar \
  --class MyClass main-application.jar

*******************************************************************************************************************************************




